{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]= os.getenv(\"OPEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 21/21 [00:00<00:00, 413.74it/s]\n",
      "Generating embeddings: 100%|██████████| 33/33 [00:01<00:00, 29.85it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(index = index, similarity_top_k=4)\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever, node_postprocessors=[postprocessor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"what is yolo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO is a system for object detection that uses a single convolutional network to predict bounding boxes and class probabilities directly from full images in one evaluation. It is designed to be fast, processing images in real-time and achieving high mean average precision compared to other real-time systems. YOLO reasons globally about the image during both training and testing, encoding contextual information about classes and their appearance. It is a unified model that can be trained directly on full images, optimizing detection performance end-to-end.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: YOLO is a system for object detection that uses a\n",
      "single convolutional network to predict bounding boxes and class\n",
      "probabilities directly from full images in one evaluation. It is\n",
      "designed to be fast, processing images in real-time and achieving high\n",
      "mean average precision compared to other real-time systems. YOLO\n",
      "reasons globally about the image during both training and testing,\n",
      "encoding contextual information about classes and their appearance. It\n",
      "is a unified model that can be trained directly on full images,\n",
      "optimizing detection performance end-to-end.\n",
      "______________________________________________________________________\n",
      "Source Node 1/4\n",
      "Node ID: 3815b6ad-ecfd-41b4-a72f-c673ab9d9a1e\n",
      "Similarity: 0.7786951002904197\n",
      "Text: Using our system, you only look once (YOLO) at an image to\n",
      "predict what objects are present and where they are. YOLO is\n",
      "refreshingly simple: see Figure 1. A sin- gle convolutional network\n",
      "simultaneously predicts multi- ple bounding boxes and class\n",
      "probabilities for those boxes. YOLO trains on full images and directly\n",
      "optimizes detec- tion perfor...\n",
      "______________________________________________________________________\n",
      "Source Node 2/4\n",
      "Node ID: 8b35ab79-acd0-440f-b0ce-b004c118d402\n",
      "Similarity: 0.7765102701777291\n",
      "Text: Y ou Only Look Once: Uniﬁed, Real-Time Object Detection Joseph\n",
      "Redmon ∗, Santosh Divvala ∗†, Ross Girshick ¶ , Ali Farhadi ∗†\n",
      "University of W ashington ∗ , Allen Institute for AI † , Facebook AI\n",
      "Research ¶ http://pjreddie.com/yolo/ Abstract W e present YOLO, a new\n",
      "approach to object detection. Prior work on object detection\n",
      "repurposes classiﬁers...\n",
      "______________________________________________________________________\n",
      "Source Node 3/4\n",
      "Node ID: 0e186821-f0e5-4349-999f-b738a8f8ae9f\n",
      "Similarity: 0.7626692571823085\n",
      "Text: Poselets RCNN D&T Humans DPM YOLO (a) Picasso Dataset precision-\n",
      "recall curves. VOC 2007 Picasso People-Art AP AP Best F1 AP YOLO 59.2\n",
      "53.3 0.590 45 R-CNN 54.2 10.4 0.226 26 DPM 43.2 37.8 0.458 32 Poselets\n",
      "[ 2] 36.5 17.8 0.271 D&T [ 4] - 1.9 0.051 (b) Quantitative results on\n",
      "the VOC 2007, Picasso, and People-Art Datasets. The Picasso Dataset\n",
      "eval...\n",
      "______________________________________________________________________\n",
      "Source Node 4/4\n",
      "Node ID: 29ca99b6-e7cc-4dc3-a603-2c4904427194\n",
      "Similarity: 0.7617917200653667\n",
      "Text: making predictions. Unlike sliding window and region proposal-\n",
      "based techniques, YOLO sees the entire image during training and test\n",
      "time so it implicitly encodes contex- tual information about classes\n",
      "as well as their appearance. Fast R-CNN, a top detection method [ 14],\n",
      "mistakes back- ground patches in an image for objects because it can’t\n",
      "see ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response import pprint_utils\n",
    "pprint_utils.pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are a type of network architecture that is based solely on attention mechanisms, eliminating the need for recurrent or convolutional layers. They are designed for sequence transduction tasks and have been shown to be superior in quality, more parallelizable, and faster to train compared to traditional models.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    #load documents and create index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "\n",
    "    # pstore for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    #load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context=storage_context)\n",
    "\n",
    "\n",
    "#we can query the index now\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"what are transformers\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
